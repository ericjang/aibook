---
chapter-number: 1
title: My Journey into AI Research
link-citations: true
reference-section-title: References
---

This chapter explores how I came to 



Artificial Intelligence, or A.I. for short, means so many different things to different audiences, that nearly every book on AI begins with the author's personal definition of it. Let me begin with my own. 

Artificial Intelligence is a broad umbrella term for multiple scientific fields that study how intelligence works, combined with multiple engineering disciplines that seek to replicate those intelligent capabilities. Much like how "cancer researchers" have not yet cured cancer yet, "Artificial Intelligence" is a purely aspirational name - we still don't know how to build a machine with human-level intelligence, even though there have been many ideas proposed on how to build aspects of that intelligence.

The umbrella of AI research spans ideas from computer science, neuroscience, philosophy, operations research, physics, philosophy, ecology, sociology. To put it bluntly, “AI” is vaguely about “computers replicating smart stuff that humans do”. What constitutes “smart stuff” tends to be a moving goalpost; engineers will automate an intelligence task thought to require general intelligence, and upon realizing that the automaton still does not posess human-like consciousness, decide that the task never required general intelligence. To quote a tongue-in-cheek definition by Larry Tesler: “Artificial intelligence is whatever hasn't been done yet.”

Some experts think that the term "AI" should be abandoned altogether, because its broad scope and aspirational connotations of being "as good as human" tends to confuse or scare the public. Some would prefer to refer to more technical subfields like "computer vision" and "Bayesian statistics". I don't share this view, because focusing too closely on present descriptors biases one to think about methodology and make public science communication more difficult. The core algorithms underpinning a particular system says nothing about the capabilities of the system, especially when learning is involved. For instance, a modeling technique such as "neural networks" can be used for forecasting housing prices based on square footage. The same technique can be used to best the human champion at Go. The term "Artificial Intelligence" captures the right aspirational goal - we seek to understand our own intelligence and replicate it in software. Also, many children become interested in robotics and AI at an early age and lack the words to describe anything more precise than that - would you really nitpick them on their vocabulary?


# 1999: Saturday Morning Cartoons

My first exposure to robots and AI was when I was about 5 years old, and watched "The Iron Giant" for the first time on VHS tape that my mom bought from Blockbuster. The tape came with a promotional "Iron Giant" figurine with moveable arms and legs that quickly became my favorite toy. I was disappointed that the robot couldn't transform into the "destroyer mode" like in the movie, but it still was one of my favorite toys.

<!--TODO: include a picture of iron giant toy on vhs -->

In the ensuring years, I would continue to nurture my fascination for robots and technology - I watched TV shows and movies like "The Zeta Project", "Bicentennial Man". 

I built short, stumpy figurines out of LEGOs and pretended that they were robots built for the amusement of other stumpy figurines (which I pretended were aliens).

<!-- do you still have legos to show a Fete -->

I even played the robot-themed *Mirrodin* set in the card game *Magic The Gathering*. I like robots a lot. Even today, much of my imagination and ambition for creating "robot people" comes from fond memories of childhood cartoons.

(mention the robot with 3 capacitors spinning that you tried to find parts for from radio shack).

 
# FIRST Robotics

I attended a public high school in California known for its competitive and college admission-obsessed culture. I did a lot of extracurriculars - I was involved in FIRST robotics, wrote for the school paper, volunteered for the national honor society, studied for the USA Math Olympiad, played on our school's water polo team, took weekly cello lessons and played cello for the California Youth Symphony. 

The extracurricular activity I was by far the most passionate about, however, was the annual SYNOPSYS science fair. It was an outlet with which I could challenge myself beyond what high school had to teach and learn about the fascinating world of "research". 

After brainstorming ideas for a long time, one day I stumbled upon the Wikipedia article for telomeres. Telomeres are protective DNA caps at the end of chromosomes that prevent them from fraying at the ends, much like how aglets protect shoelaces. Due to a quirk of how chromosomes split during mitosis, cells eventually run out of telomeres after dividing too many times, leading to programmed cell death. However, Cancers and certain simple animals are actually able to bypass this limit, allowing them to replicate indefinitely. I found this idea of "biological immortality" fascinating - 



However, after volunteering in a wet lab at UCSF to research yeast telomeres, I came to realize that biology research was slow and arduous, and not conducive to a fast-growth career. Only a fraction of postdoc time was spent thinking about the science and designing experiments. The majority of time was spent pipetting liquids into PCR plates, making gel media, inoculating petri dishes, and generally moving liquids around between test tubes. 



Although the science I did at the time was probably full of errors and not groundbreaking at all from professional standards, I credit this period as the *true* start of my career.



It was a crazy time, and probably the hardest I've ever worked in my life. Everything since then has been relatively low-stress.

In 2012, as a senior in high school, I began writing down "Strong AI Manifestos" on how we could build AGI - these were grandiose and yet imprecise outlines for what we needed. 


<!-- My first exposure to robots and AI was when I was 6 years old during my Saturday morning cartoons, when I would watch *The Zeta Project*. The show was about an intelligent robot, nicknamed "Zee", that was originally built for assassinations but subsequently comes to realize the value of life and as a result, abandons his mission and becomes a fugitive on the run from his human creators. The show explored themes around robot sentience and rights, but as a 6 year old boy, I was far more excited about the arm lasers and robot spy battles. 
 -->
<!-- In the ensuing years I would learn to appreciate that movies about robots are less about cool Gundam-style fights, and more about holding up a mirror to Humanity itself - what separates us from a soulless machine? Works like *Bicentennial Man* and *Blade Runner* and *Zima Blue* explore these themes well - by following a robot’s journey where it acquires humanity one piece at a time, the audience is reminded what it means to truly be human.  -->

<!-- It is the simple things in life -human freedom, human love, and finally to be recognized as human.  -->


I soon came across Jeff Hawkin's book "On Intelligence" 

I soon became interested in neuromorphic brain simulation, after watching Kwabena Boahen's TED talk on developing a power-efficient chip that could simulate a billion spiking neurons in real time. I was blown away, and emailed Dr. Boahen to volunteer for free in his lab. That summer, I learned how to 


# Blackburn Lab

# Neuromorphic Chips and Biophysical Neuron Simulation

- implemented neural networks and tried to implement simple learning algorithms like STDP





# AI and The Media 

News and Media companies make their money from advertising. In other words, their revenues are based on how many eyeballs are looking at their webpages. Driven by these incentives, journalists tend to craft sensationalist headlines about technology to provoke emotional responses. This has the unintended consequence of exacerbating the public’s misunderstanding about the how and what of AI. Consider the following headlines about recent research:

- "It will change everything": DeepMind’s AI makes gigantic leap in solving protein structures (Nature)
- Google’s AlphaGo AI defeats world Go number one Ke Jie
- OpenAI's New Multitalented AI Writes, Translates, and Slanders
- Facebook AI Robots Shut Down After They Secretly ‘Invent Their Own Language’
- Elon Musk: ‘Mark my words — A.I. is far more dangerous than nukes






There are a surprisingly large community of intellectuals and "AI Philosophers" who fan the flames of media sensationalism by making wild extrapolations of current AI systems. There is a flavor 


1) the difficulty of the average human to comprehend exponential growth, especially when it comes to technological trends. 2) 


The basic argument is based on two assumptions:

1. Base case: an AI that can do anything better than a human can, including the design of AI.
2. The inductive step: An AI that is “better” than another AI at a task (such as designing AI) can do it better in every way (speed, optimality).

Once an AI system reaches human-level intelligence, it would be able to design and improve itself at least as well as humans can, but is now unfettered from the limits of human biology so it will exponentially increase in capability and humans will lose control of the technology. The exponential growth reaches a near-vertical slope, intelligence explodes, and this event is known as “AI Singularity”. 

Some philosophers like Nick Bostrom argue that the potential future danger of a God-like “Superintelligence” is so vast that from an expected-value perspective, we should be investing a large amount of philanthropic work on mitigating risks associated with Superintelligence.



From reading articles like these, you might come to believe that we are on the cusp of a technological revolution that is so great that 

humanity's monopoly on intelligence is about to end. On the other end of this spectrum, you might see news headlines designed to provoke fear:






These headlines insinuate the narrative that "machine minds are coming to replace our jobs” or "world-ending robots -- like SKYNET portrayed in the Terminator movies -- are right around the corner".

Even a relatively mundane headline like “Artificial intelligence solves Schrödinger's equation”, while not technically inaccurate, can still be misleading due to the duplicitous nature of the English language. To the researchers who built the system, “Solve” was intended to be used in the same way that “a calculator solves a differential equation”: nothing more than a rote but expensive calculation that is better done on a computer than by hand. 

However, the use of the term “Artificial Intelligence solves” gives the headline a different meaning: by anthropomorphizing the AI, the word “solve” evokes the imagery of a lone genius deriving a brilliant, original solution to a century-old problem on a blackboard. The personification of computation is described much like how "Andrew Wiles Solved Fermat's Last Theorem", in contrast to the rote simulation and calculation our AI systems actually perform.

These headlines, in which all expert nuance is discarded for mass consumption - highlight the reality that public science education is woefully inadequate for understanding modern technology and mathematics. This is not unique to AI - we see this happen often in quantum physics (Does Quantum Physics Support Buddhism?), biotechnology research (Stem Cells Reverse Aging). There are publications (Quanta Magazine) that attempt to bridge the knowledge gap between the experts and the public and bring a more accurate representation of science to mainstream, but by and large the public does not understand the limitations of computer technology and modern AI research.




In academia, PhD students work mostly independently 

<!-- Being an AI researcher is not only about building systems that accomplish intelligent things, but because our understanding of intelligence is so poor, we spend a lot of time asking "why" questions.

Each time I run an experiment, I look at the data - if there are any surprising observations that don't match up with my understanding of reality, then I formulate a hypothesis to explain that anomaly, and then implement a new experiment to prove or disprove the hypothesis. Rinse and repeat.
 -->


- Numenta whitepaper
- Strong AI manifesto - excerpts
- 

Early on, I had started to formulate some intuitions about the "right" way to build a human-level AI. 

I felt that building a sort of "baby AI" and raising it like one would a child would eventually result 




<!-- 



 Like all programmers, we regularly search on Stack Overflow for how to do mundane things like convert images into gifs or adjust a plot to be more aesthetically pleasing. 

Some programmers like to read tech news, like techcrunch.com or news.ycombinator.com, to start their day. Similarly, many AI researchers will have internal chat groups or social networks where they share recent papers posted on Arxiv, a preprint website where the majority of AI research is published before it is accepted at a conference. Normally in other fields people would wait for a conference or journal to accept the paper prior to submission, but the field moves so fast, and there are often so many people working simultaneously on the same ideas that there is a zero-sum game: "if I don't publish as fast as possible, then someone else will be able to publish on a preprint server and claim they published first!"

 -->

<!-- Contrary to how the media reports scientific breakthroughs, there are no turtleneck-wearing, equation-writing professors who "crack the code". There is certainly math in spades, but AI research is as much an engineering discipline, since great care is needed to implement the ideas on the whiteboard correctly.
 -->



In my 5 years at Google, I worked on the first end-to-end RL systems, scaling up imitation learning in our lab, and making some advances in more basic methods research in probabilistic modeling. I watched as many colleagues around me grew into their respective research niches, becoming more experienced research
I look over yesterday evening's experiments

The robots are doing the same things as they were yesterday, which is often nothing at all, having halted for some reason or another from yesterday.




On any given day my computer screen has a window where I write code, another window where I schedule experiments on a supercomputer, and a browser full of tabs where I write emails to colleagues, look at experimental plots to deduce what's going on, and write down my experimental observations and hypotheses.



AI researchers often should switch between two modes of thinking:

high-level thinking, in which we think broadly about how to put things together and turn it into a capable system. the underlying ideas in alphago (self-play) this book, are all high-level ideas. 

the low-level thinking is about making the ideas actually work -the execution, the debugging, thinking deeply about stable numerical optimization, thinking about the best way for a neural network to spit out a quaternion for representing orientations.


 moves the field forward.




In 2015, Eric Schmidt gave an interview at Stanford School of business where he recalled how Google was able to triple their revenues overnight by switching to a software-based ad auction system instead of using a human sales team to price their advertising products. He predicted that a new kind of programmer, aided by Machine Learning, would be able to scale businesses even faster

that the future would see a new kind of programmer who would be able to scale 




The newest breed of AI researcher - the sort that has contributed the most breakthroughs in the last two decades - works on a broad form of computational statistics called “Machine Learning”. Some AI researchers develop new algorithms to solve open-ended problems to make their systems more efficient, while others work in more application-heavy settings like Netflix’s recommender system or Google’s text-to-speech assistant. 


Separately from that are different fundamental tools, like statistics and ML.


The general public is generally unaware of the mundane details of software programming, so they rely on the science fiction portrayals of “AI” to shape their understanding of the technology. It comes as not surprise then, that the general populace  reacts to any sufficiently advanced technology with exuberant optimism or unbridled fear. 
To be clear - to date, no one has created AI that reflects a degree of “agency” that is as convincing as what we see in science fiction. How we do that will be the subject of this book.
 
In science fiction and public discourse, “AI” has very different connotations. A “General AI” is an  “Artificial” being capable of doing anything a human can - much like an actual person. This is reinforced by science fiction portrayals, where AIs range from intelligent assistants to murderous robots. These robots often possess an anthropomorphic sense of self,  referring to themselves as “I” or having self-preservation as a part of their programming directives. 
Simultaneously, people have deep-seated fears of being replaced, of losing purpose in life because someone or something can do it better than they can.  
 
In order to gain a deeper understanding of AI technology, we must discuss how we build these sophisticated computer programs. 
But in order to create Artificial Intelligence, we will have to get more precise than that.
 
However, without an understanding how AI systems are built, it becomes very hard to reason about the system’s actual capabilities and limitations. It would be like trying to discuss the future of vehicles without knowing the efficiencies of combustion engines, the aerodynamics of planes, or the physics of energy. Much like car designers and manufacturers think about how to make drive chains more efficient or rubber more durable, AI researchers and engineers traffic in getting Machine Learning systems to scale to larger datasets. Fortunately, the first part of this book is dedicated to gradually building up your understanding of how AI works, the most promising aspects and the most challenging bottlenecks.


With my machine learning and robotics colleagues, we use “AI” to casually describe the catch-all term for “sufficiently advanced software”, knowing full well that such a term is so devoid of technical precision that it isn’t actually useful when describing how to build actual systems. Sometimes experts will refer to a system as “AI” because we are deliberately dealing in the abstract - we don’t know how exactly it will be implemented, so we may refer to hypothetical systems as an “AI” rather than a “Machine Learning system”. 



# Software Today
<!-- 
“Intelligence” is a word casually used in everyday conversation to describe the behavior of people and animals, and yet its meaning is rather slippery; we don’t know how to define it precisely enough that we can type it up into a computer program. This chapter serves as a gentle introduction to what “AI” is, and clear up the most common misconceptions around the technology.


, especially tasks thought to require animal or human-level intelligence.  For AI researchers and computer scientists, “smart” is something of a moving goalpost. 

“Intelligence” is a word casually used in everyday conversation to describe the behavior of people and animals, and yet its meaning is rather slippery; we don’t know how to define it precisely enough that we can type it up into a computer program.

 -->
Many investors and business leaders buy into sensationalist narratives that AI is a quasi-magical software that can automate nearly anything. This is not because they are intentionally peddling snake oil, but because they simply lack the technical expertise to decide what kinds of software operations are “simple” and which ones are “advanced”.
<!-- 
Software systems today can play Go better than humans, diagnose breast cancer better than most human radiologists, transcribe our voice clips to text, manage air traffic congestion, and handle the back-office paperwork for employee vacation hours. -->

I’d like to relay a personal story that highlights these risks. A friend of mine has parents who run a successful online SAT prep school in the USA. They incorporated web technology into their business in the early 2000s, and investing in this up-and-coming technology paid off well for them. Owing to this success, they were keen on staying up to date with newer technological fads like “AI” and “Blockchain”. I met them for tea one afternoon in San Francisco, where they regaled me with their grand vision of using AI to generate test questions and adapt to changing Collegeboard formats and giving each student a customized learning curriculum.  

Right away I could tell that they were savvy with the business importance of test material, but lacked the technical understanding about AI technology to achieve their stated goals. I gently told them that while there were some promising breakthroughs in the Natural Language Understanding, this was an unsolved problem of unprecedented difficulty that no present computer program could do. 

I gave them a few examples of technical hurdles that only someone deep in the weeds of implementation would be aware of: 

- Current AI technology can be trained to generate plausible-sounding questions on their own, or plausible-sounding answer choices on their own. But no technology exists to line up “plausible questions” with “plausible answers” and “plausible false answers”. 
- When “re-writing” a question, do the transformations incorporate additional common-sense knowledge of the world? What does common sense even mean in the standardized testing context? 

A simple task that any native English speaker could do - migrating SAT questions to comply with a new set of standards - abounds with hidden complexity when we try to replicate it in software and really write down all the instructions a human would need to do to handle the task. I told my friend’s parents that even if they spent millions of dollars in R&D, and hired the strongest team of researchers to build a system for studying this problem, it would be highly uncertain whether the task could be solved to an adequate degree in 2-5 years.

# 2016-2021: Google Brain

For all the sensationalism of AI technology, the daily routine of an AI researcher is surprisingly similar to that of your typical software engineer in Silicon Valley. I usually wake up around 7:30, brew myself a strong cup of coffee, and have about two hours of undisturbed coding time where I can think at my best.

Do I spend my most productive hours of the day lost in thought about how one goes about imbuing machines with self-awareness? Not really. As a scientist, my work is mostly about tackling big questions by breaking them into smaller, actionable ones that can be tested with *empirical experiments*. 

To use an example from rocket science, here's one of the "big science questions" from astrobiology: what would human life and civilization look like on Mars? A smaller question might be: how might we construct a life support systems to Mars? How many people would it support? Breaking down those questions even further, one might ask "what is the heaviest system we could ship to Mars with current rocket technology? Could we make that number better?" 

Go deep enough, and science questions start to look like engineering. For example, if you are trying to build a rocket to take life support systems to Mars, one question you might encounter on the day-today is to understand why your experimental results don't match the performance of a theoretical rocket. In the process of answering the "why" question, you end up making systems that are stronger, faster, better.

AI research is much like rocket science in that regard. We all enjoy talking about the big, blue-sky questions about the nature of consciousness and whether it is possible to love a machine, but in order to get there, we have to answer a lot of smaller questions through a lot of careful engineering. Often my mornings are spent figuring out why something I coded up the previous afternoon isn't working as expected. If there are surprising results, then I might attempt to run follow-up experiments to really convince myself that the experiment wasn't a fluke.

<!-- In robotics, what can seem like simple  abounds with edge cases. This "hidden complexity of everyday intelligence" will be a recurring theme in this book. -->

Later on in the day, I'll check emails from colleagues who have sent me code changes to review. In industry research, it's far more common to work in a collaborative manner, where we check each other's work and build infrastructure that benefits multiple large research efforts. I receive lots of other kinds of email, but I've found that simply ignoring those emails has had about the same effect on my career growth as reading them and maintaining an empty inbox.

Throughout the day, I'll meet with colleagues to present a status update on my research and what I'm currently stuck on. The nature of research tends to be a state of perpetually being stuck, followed by brief periods of being un-stuck, followed by getting re-stuck.

Around 6pm, I'll close my work laptop and fix dinner for myself, and work on one of my hobbies like blogging or coding for a side project or researching investment ideas. At 11pm, right before I go to bed, I might double check to make sure that my experiments are still running, and kick off any last minute experiments that I'd like to have ready for me in the morning. 

# Present Day


<!-- talk about your decision to step away from robotics-something that you are deeply passionate about - to start over,

re-thinking the research model from one of academia to one that is more tightly integrated with AI products.

robotics often tries to 1) build the robot 2) have the robot leave the lab. 

But what about doing the other way around, 1) having some software AI leave the lab, and then 2) embody it afterwards?



why now? AI is leaving the lab, and it might be the case that you have to leave the 

 focusing on the data and scaling purely in software.

w
-->


<!--
# On Rationalists

Fanned by the flames of media sensationalism, there is a surprisingly large community of pseudo-intellectuals and “AI philosophers” who will make wild extrapolations on the capabilities of AI systems. The basic argument is based on two assumptions:

1. Base case: an AI that can do anything better than a human can, including the design of AI.
2. The inductive step: An AI that is “better” than another AI at a task (such as designing AI) can do it better in every way (speed, optimality).

Once an AI system reaches human-level intelligence, it would be able to design and improve itself at least as well as humans can, but is now unfettered from the limits of human biology so it will exponentially increase in capability and humans will lose control of the technology. The exponential growth reaches a near-vertical slope, intelligence explodes, and this event is known as “AI Singularity”. 

Some philosophers like Nick Bostrom argue that the potential future danger of a God-like “Superintelligence” is so vast that from an expected-value perspective, we should be investing a large amount of philanthropic work on mitigating risks associated with Superintelligence. A outcome that these futurists take rather seriously would be that such a “superintelligence” will become so powerful that mispecifying the objective “make paper clips efficiently” will result in all living organisms being harvested for the iron in their blood as the runaway AI turns the entire solar system into paper clips. This situation sounds absurd, but even if the probabilities are small, the superintelligence crowd believes that when multiplied by the near-infinite human suffering of this outcome, the resulting expected-value calculation is still greater than any other humanitarian issue worth working on.

It’s no coincidence that the set of people who think that AI safety tend to be the most privileged, ivory-tower types of society, who seldom think of more wordly issues that technology brings to society. It’s also no coincidence that these people spend a lot of time doing policy writing rather than writing code and uncovering new truths about AI.

The problem with the “AI Singularity” hypothesis is that it is a shoddy appeal to the language of mathematical induction and statistical thinking - there are no precise mathematical quantities in the above statements, so the “proof” is merely dressed in a veneer of mathematical authority. For instance, when we say that an AI system can improve itself as well as humans do, what do we mean by "improve"? 

Infinite future suffering is still infinitely bad, no matter what probability you multiply it with, and no matter how you estimate your confidence intervals. Frequentist statistics breaks down when reasoning about events that can happen once, such as the extinction of humanity. Even if one interprets *probabilities* 

In this book I will make no attempt to hide my disdain for philosophers and “rationalists” who make a lot of noise and tour the media circles but do very little to advance the technology themselves. Rationalists pride themselves on adhering to Bayes rule - then I challenge them to explain what evidence would update their belief to no longer think Superintelligence is no longer a risk. If they cannot think of any such evidence, it means their claims are not falsifiable, and therefore the topic of philosophy and not science. 

A philosophical interest in quantum physics does not make one an authority on quantum physics, which is entirely what is happening with the rationalist community’s perspective on AI. It’s the experimenters who probe reality that hold the truth, not the people who sit in an armchair who philosophize on what intelligence and can and cannot do.
Coming for our Jobs
-->

<!-- 

Summary
There are two meanings when people use the word “AI”. The first is from science fiction, and carries with it anthropmorphic bias of robots with their own agency. 
The latter is “narrow AI”, which is a moving goalpost dictated by how “hard” the technology is. What was considered AI 20 years ago is just “regular code” today, hardly thought of as "innovation".

Only the engineers on the ground have a nuanced view on what is “hard”, so everyone from VCs to CEOs confuse the public by m their products “AI”. 
The media simplifies language that evokes anthropormophic bias towards AI. 
AI Alarmists ( a disjoint set of people from AI researchers who actually move the technology forward) genuinely believe that Superintelligence Risk is one of the greatest threats facing humanity in the future and we need to act on it today. -->
