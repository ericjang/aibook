---
chapter-number: 13
title: Language and Thought
link-citations: true
reference-section-title: References
---
*“A man is what he thinks about all day long."*
*— Ralph Waldo Emerson*



What would it take for agents to sit and think all day long? When did animals start to do this? 


The development of language must be embodied - agents come up with new words to express new ideas, and new ideas can be used to develop things that don’t exist yet, or provide a framework for seeing the world more clearly.



The question “What is it?” is a question that many animals ask themselves when confronting the unknown, but the question does not form into words until the agent must converse with another agent which might know the answer “what is it?” 


agents start to ask questions of each other, coupled with a drive to learn more about the world.
Agents can ask more wise agents for information.
How to get the evolution of creatures that take care of each other?


Language probably helps mediate complex social dynamics 

https://en.wikipedia.org/wiki/Behavioral_modernity

But at this point, let’s assume we have 1) a powerful algorithms and infrastructure to optimize virtual agents 2) agents that can make use of tools 

An area in ventral occipitotemporal cortex of left hemisphere is called Visual word form area (VWFA) - recognizes objects and symbols (e.g. a house and a drawing of the house). Around age 7 in reading cultures, it responds more strongly to written words. Same regardless of logographic or alphabetic languages. 

Acquisition of language → takes months / years of practice. Does this imply we need to simulate creatures for months and years? 

Is it possible for substantial mastery of language to be in-born, or is there something advantagoues about the way in which humans must spend a lot of time to acquire such skills? (for instance, the ability to learn other really complicated things)



Ability to read → vastly expand information storage capability.

 This is distinct from the ability of social animals to call out danger when they see it - this would be discussing danger before it has happened. 

We already have neural networks that can read well. But the brain is a meta-learning system, and the reading capability of the brain is a byproduct of a learning system that develops reading as needed.







AI systems that can teach us things and explain how the world works to us.

Information is nothing more than the existence of a pattern. The absence of a pattern is nothing more than randomness, or the lack of information.

Coincidentally, P-A-T-T-E-R-N was the first “big word” I learned to spell in kindergarten -- with pride, I might add. Much to my disappointment, my far more gifted twin sister could spell the C-H-O-C-O-L-A-T-E, a word with so many letters I could scarcely comprehend it)  it would be many years before I learned to write code and understand the mathematics of information theory, but my interest in robots did emerge in a very early age. 

Internal consistency



Simulation will have to be extended so that there is a medium for transcribing thought to a durable format (exploiting materials in the environment, understanding tool use).



It’s posible that solving language is a pre-cursor to solving robotics. Aspects of longer-horizon tasks are cognitive in nature, suggesting that if you had a good cognitive-level reasoning system (cause and effect, describing the physical dynamics of the world in words), then you can leverage that knowledge to search over actions, reason about consequences of actions, and plan accordingly.






Reading a children’s book to a virtual child and seeing how they react

At a certain level of ability, we can no longer just observe behavior - we need to monitor emergent phenomena like thought. Language that communicates said belief, and using said language to implement “thought”.


Explaining concepts

Building virtual scientists that question the world, form beliefs. Thought begets belief, which begets individuality, which begets the sanctity of life.




