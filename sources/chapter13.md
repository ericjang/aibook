---
chapter-number: 13
title: Language and Thought
link-citations: true
reference-section-title: References
---
*“A man is what he thinks about all day long."*
*— Ralph Waldo Emerson*

<!-- TODO: use the bolt-on generalization essay ?-->

To imbue our agents with really profound levels of generalization, we may have to rely on the learned structure of "internet-scraped datasets"



Previous chapters discussed how a single "game transformer" model can be language-conditioned to facilitate shared learning of different games and agent behaviors. We now consider how language can emerge not as a conditioning mechanism for the model, but as an action taken by the agents themselves.

Language is *not enough* to convey meaning in a watertight way (see chapter 5), but coupled with the behavior of agents that share *understanding* through their behavior and implicit understanding of each other, language does facilitate "collaborative thinking" between individuals.





What is the minimum complexity environment that would allow an agent to form hypotheticals about the world?

<!-- priority: high. this chapter is very unpolished, not sure what layout will be -->

<!-- open up chapter with brief notes on the neurobiological stack around language, like broca's area -->

<!--Sapolsky on language -->
<!--https://www.cjfearnley.com/G+Posts/20150710%20-%20Robert%20Sapolsky%20on%20Language_%20this%20video.html-->

meaning is discrete - Noam chomsky points out that you cannot have "half" a word, a word is a discrete unit of "meaning", whereas sound can be continuous.

language is an efficient way to compress meaning. From an information-theoretic perspective, discrete codes are better for communicating information over noisy channels.


Cultural memory and stories serve to teach a social animals how to function, just as hippocampal memory teaches a squirrel where it buried its nuts. 


So far, we have focused on the neurobiological basis of behavior and how to set up training environments so that agents learn to "behave in ways that reflect understanding of themselves and other minds".

# Neurobiological Basis of Behavior

At 9 months of age , infants begin babbling and imitating the spoken sounds in their environment (mother tongue). 

We know that there are elements of the human brain that appear to be specialized for language. Language is the most lateralized thing, in addition to handedness - it happens on one side of the brain. Researchers have determined, via anesthatizing people's side of brains, 90\% of people have language lateralized to the left side of their brain


three regions in cortex relevant
broca's area (in parietal lobe). Bottommost part of cortex that moves body, specifically moving lips (language production, sign langauge production). 

Broca's area being damaged is a "production aphasia"


On the other hand, Many people with specific language impairments like Williams-Beuren syndrome also tend to have lower IQs, suggesting that language is not a distinct module in the cortex, but rather interacts in complex ways with other "intelligence modules".
However, this remains a controversial issue even among researcher.


All animals think to some degree, in order to survive. But under what conditions and ecological nicehs would an animal spend all day thinking about something?

For one, its immediate Maslovian needs have to be satiated - it can't be in imminent danger or have to make some urgent, time-sensitive action.

As we scale up the Jungle Football game, to one that potentially lasts years in simulated time, we give our agents the capacity to think about 


pre-training on large amounts of language imbues agents with basic "thinking" capacity. Can use this as a module for agents to "reason" about the world, as one would reason verbally.

What would it take for agents to sit and think all day long? What compelled the first animal to do this?


The development of language must be embodied - agents come up with new words to express new ideas, and new ideas can be used to develop things that don’t exist yet, or provide a framework for seeing the world more clearly.



The question “What is it?” is a question that many animals ask themselves when confronting the unknown, but the question does not form into words until the agent must converse with another agent which might know the answer “what is it?” 


agents start to ask questions of each other, coupled with a drive to learn more about the world.
Agents can ask more wise agents for information.
How to get the evolution of creatures that take care of each other?


Language probably helps mediate complex social dynamics 

https://en.wikipedia.org/wiki/Behavioral_modernity

But at this point, let’s assume we have 1) a powerful algorithms and infrastructure to optimize virtual agents 2) agents that can make use of tools 

An area in ventral occipitotemporal cortex of left hemisphere is called Visual word form area (VWFA) - recognizes objects and symbols (e.g. a house and a drawing of the house). Around age 7 in reading cultures, it responds more strongly to written words. Same regardless of logographic or alphabetic languages. 

Acquisition of language → takes months / years of practice. Does this imply we need to simulate creatures for months and years? 

Is it possible for substantial mastery of language to be in-born, or is there something advantagoues about the way in which humans must spend a lot of time to acquire such skills? (for instance, the ability to learn other really complicated things)



Ability to read → vastly expand information storage capability.

 This is distinct from the ability of social animals to call out danger when they see it - this would be discussing danger before it has happened. 

We already have neural networks that can read well. But the brain is a meta-learning system, and the reading capability of the brain is a byproduct of a learning system that develops reading as needed.







AI systems that can teach us things and explain how the world works to us.

<!-- Information is nothing more than the existence of a pattern. The absence of a pattern is nothing more than randomness, or the lack of information.
 -->
<!-- Coincidentally, P-A-T-T-E-R-N was the first “big word” I learned to spell in kindergarten -- with pride, I might add. Much to my disappointment, my far more gifted twin sister could spell the C-H-O-C-O-L-A-T-E, a word with so many letters I could scarcely comprehend it)  it would be many years before I learned to write code and understand the mathematics of information theory, but my interest in robots did emerge in a very early age. 
 -->

Simulation will have to be extended so that there is a medium for transcribing thought to a durable format (exploiting materials in the environment, understanding tool use).

It’s posible that solving language is a pre-cursor to solving robotics. Aspects of longer-horizon tasks are cognitive in nature, suggesting that if you had a good cognitive-level reasoning system (cause and effect, describing the physical dynamics of the world in words), then you can leverage that knowledge to search over actions, reason about consequences of actions, and plan accordingly.


Reading a children’s book to a virtual child and seeing how they react

At a certain level of ability, we can no longer just observe behavior - we need to monitor emergent phenomena like thought. Language that communicates said belief, and using said language to implement “thought”.


<!-- Explaining concepts -->

<!-- Building virtual scientists that question the world, form beliefs. Thought begets belief, which begets individuality, which begets the sanctity of life. -->

## Open questions

generative model of input, FMRI scan, human output. See if this model can "learn to think"


