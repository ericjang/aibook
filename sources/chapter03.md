---
chapter-number: 3
title: Power and Control
link-citations: true
reference-section-title: References
---

Now that we’ve discussed how great neural nets are and all the functions they can approximate, you may be tempted to think that all of society’s technology problems can be solved with deep learning. Let’s just collect a lot of data, fit it to large neural networks with an abundance of compute, we can create any software imaginable!

Indeed, we have many reasons to be optimistic about the technology and the principles underlying end-to-end machine learning. Deep learning has allowed us to scale to larger input modalities, more data, bigger models, and capture decision making that we struggle to articulate with our feeble Software 1.0 assumptions. However, in this chapter we should be realistic: deep learning does not equate to “technological progress without limit”. 

If I were to summarize the developments in machine learning in the last decade, it would be that we have made very powerful systems using end-to-end optimization with as few assumptions as possible. We give up our assumptions of how software should work, and let automated search find the best solution for us. 

The trouble about giving up our assumptions when building end-to-end ML systems is that by relinquishing control on how the ML model makes decisions, the solution that Software 2.0 finds may be completely inconsistent with how we think the ML model should make decisions.

. For example, suppose you have a dataset of stop signs and other traffic signs. From this dataset, an end-to-end ML system may decide that anything reddish in color is a stop sign, and everything else is not. But it ignores the octagonal shape of stop signs, the word “STOP”, and so forth. So if you showed it a picture of a fire extinguisher, the ML model might misclassify it as a STOP sign (a false positive error). 

One might design a more clever training dataset to remove such “spurious correlations” (for example, training the network on red, octagonal objects that are not stop signs, but in practice it’s hard to design examples to cover all the edge cases.

Having less transparency on how ML models come to make the decisions they do is, at best, hard to debug, and at worst, dangerous for the users. 

By taking those assumptions away, we have lost the ability to exert control over the complex, parallel computations now being made by function approximators. There is no intermediate, human-designed output intended to be user-readable. This makes the system more powerful (often what we ask the intermediate outputs to be is not the best way for the system to solve the task), but it makes its internals also inscrutable and we lack the guarantees to know what it might predict.

Interpretability is a hot research topic. We often hope that our AI models will be able to “explain themselves”. For example, we might want to know which input values contributed most strongly to a network’s decisions.

The thing is, artificial networks are perfectly interpretable. It’s just that the computations are so numerous 

Unlike a biological mind, where we are not even sure where it begins and ends within the organism, a neural net is relatively straightforward set of computations. It begins with an input, and follows a series of parallel linear algebra routines, ultimately transformed into some output.

If you wanted a complete explanation for why a model predicted what it did, you only need to trace its input through every set of linear algebra routines until you arrive at the model. 

Similarly, if you want a complete explanation for why a model got to its current set of parameters, you only need to trace the updates generated by the dataset over the course of training. All the computations are there for you to inspect, one by one, and you can replay them over and over again exactly as they happen. 

But this is not quite satisfactory to someone who cares absout “explainable AI” - they want a simpler, compressed narrative of how a model got to that decision without worrying about every single multiplication and addition operation that took place.

So, when we say “interpretability”, we don’t mean the ability to probe a neuron like a neuroscientist does a mouse. We mean that the model should give us a fairly short summary of the millions of computations that took place, so that we get the most important parts described to us.

Of course, this may very well be an exercise in folly.


Similar to how we try to summarize the intricate computations of fluid dynamics into a simple explanation of why planes fly, we find that any intermediate, non-technical explanation sort of falls short, does not capture the whole picture, is leaky.
https://www.scientificamerican.com/article/no-one-can-explain-why-planes-stay-in-the-air/


The no free lunch theorem tells us that a model is only as good as its biases and data permit it to be. If you want to spend less money on computers or data, you had better write down some good biases. And if history is any indication, humans are not particularly good at formulating good biases for models - if we were, we wouldn’;t have any need for learning! 


“Sample-efficient learning” is an oxymoron. If you want to spend less $ on compute/data, the no free lunch theorem requires you to write down good inductive biases. If you are hard-coding more and learning less, can we really still call that “learning”? 


Execution

Firstly, the Deep Learning recipe is not very reliable because we don’t fully understand when it works and when it doesn’t. Behind every successful demonstration in the field, there are dozens of similar projects that had the right idea but failed due to execution reasons:

Not enough hyperparameter search
A missing building block not invented yet (e.g. Transformers, ReLUs, Normalizing Flows)
an intuitively correct idea but a mistake in the mathematical formulation.
Software engineering skill of the lead researcher.

The AlexNet paper which kicked-off the Deep Learning revolution was preceded a year earlier by a DNN developed by Dan Cireșan, but Cireșan’s network was considerably smaller and he never demonstrated it on a sufficiently hard benchmark. This theme continues to today in computer vision community - the entire field is mostly focusing on the exact same set of problems with tweaks to neural net architecture, so the function approximators are not doing anything substantially new.  But the difference in execution ability has since bridged the 15% top-1 error rate to less than __%. 

The little details involved in improving basic building blocks needed to realize higher level ideas. Take for instance, reinforcement learning, a decades-old effort to create methods that control software agents that perform some task and improve that agent based on experience.

DeepMind is an organization that employs over 1000 people to date, with a sizable population of its researchers working to improve reinforcement learning technology to be more efficient and  scalable.

In 2020, they published Agent57, a generic RL agent that finally matched human-level performance on the Atari benchmark, which they started working on in 2016.

Ultimately, Agent57 comprises thousands of scientist-hours and billions of cumulative compute hours in automated search to find a better Q-learning agent. Agent57 is an incredible achievement, but fundamentally solves the same problems that DQN does. Considering that the original DQN paper was published 6 years earlier, this was still an incredible number of additions to the field made possible by the efforts of the entire research community. Such is the real speed of progress, which is often glossed over by people who are enamoured with the concept of AI but are not really on the ground getting their hands dirty with research experiments.
One reason why research progress can be slow is that academics tend to get lost in the minutiae, asking low-level questions like “how do we tune the regularization term in Max-Ent algorithms to make it easier to use”. 
Such questions have driven much of the AI breakthroughs of the last decade. But 10 years from now, if we are still primarily concerned with these questions and not asking increasingly philosophical questions of identity, creativity, consciousness, and getting agents to dream as humans do, that would be a great disappointment to me. 
utilizing Deep Learning is still much of an art than a science, which means that developing something challenging like AGI will require engineers and researchers of such profound intuition and skill if they are going to rely on methods that are not fully mature.
Without clear understanding of natural data and the models we train on it, we have to rely on pure empiricism - it is effective and will eventually work, but is simply kind of slow.
 

Most academics are more concerned with claiming credit for fairly blasé ideas for creating powerful systems than actually building powerful systems.


I’ve reviewed close to a hundred papers now and read close to a thousand.

Surface Level Statistics vs. Symbolic Manipulation

Rule-based methods have often been offered up as the solution to the shortcomings of ML. They go by many names - symbolic AI, structural causal models, reasoning-based systems. ML critics such as Gary Marcus, Judea Pearl love to throw shade at machine learning, arguing that curve fitting can never capture the complexity and generality of logical operations on symbols. 

The critics rightly point out that neural networks are currently quite brittle in ways humans are not. Often, astoundingly brittle. 

Take, for example, the following task: you read some gas prices on a sign and want to compute the price change from last month. The reasonable, “hybrid” approach would be to use Machine Learning to parse the “symbols” from the image - e.g. using optical character recognition to read the price $3.70. From there, we rely on good old-fashioned Software 1.0 programming: we use a database to store last month’s gas prices, look up that price and perform numerical subtraction.

In contrast, the end-to-end approach sounds absurd. A model looks at two images and outputs the differences in prices - no intermediate subtraction or even understanding that there are numbers. This approach might eventually work with a large amount of data, but much like the Prisoners in Plato’s Cave, our current ML methods will likely never understand the true symbolic nature of the images - that each image contains a numerical price and we are to subtract the prices. 

Marcus draws an analogy to The Allegory of the Cave: people inside a cave are holding up puppets, and a fire illuminates the cave so that shadows of the puppets appear on the walls. Some prisoners are facing the walls and only see the shadow. The shadows represent observations, but the true nature of the shadows is the puppets. You can make deductions on how the shadows will move based on a lot of observation of how they tended to move in the past, but until you truly understand how the shadows are created - the existence of the puppets and the optics, your understanding will only be cursory and forever limited to what you have already seen.




Self-Driving Cars Are Still Kind Of Dumb


Cost

Deep Learning is expensive. 

<Cost OpenAI roughly $12M to train their GPT-3 system>


deep nets are very lazy predictors that can do a little better when it is encouraged to be not so lazy. The question is whether they can do something very non-lazy as we make datasets harder.


We Still Don’t Know How to Create AGI



Many researchers have called for our systems to utilize more assumptions in our models, either in replacement of or in addition to huge amounts of data. Depending on who you are speaking to, this is called “causal reasoning”,  “symbolic methods”, “interpretable models”.



knowing how to build a superhuman chess player does not mean we know how to build a human-like mind that also knows how to play chess.

The “operating system”’s role is to coordinate the execution of multiple applications simultaneously. Furthermore, there are big integrative questions here. Do we build some kind of “hierarchical” controller that decides which “application” to run? What applications do we pre-package the system with, and what if we need to learn a new application? 

Even if we had all the individual pieces of Moravec’s landscape above, there’s still the challenge of how all these “intelligence” applications coordinate with each other to realize a system that can borrow 
The process of executing some application, we receive a stream of information. 

these capabilities sometimes feed into one another to enable each other. For example, memory can aid learning. When we sleep at night, our brain replays memories to us - jumbled together in the form of a limited consciousness experiencing dreams. Dreaming is thought to be important to the learning process. But if these pieces are dependencies of each other, how do we approximate these functions separately?
Is Common sense reasoning an individual module, or an operating-system piece that talks to other modules? Does it even make sense to impose a hierarchical control scheme where the “operating system” asks various pieces (curiosity, planning, etc) to perform their specialized function, like a conductor at an orchestra? Or is it just a cacophony of neural regions, like a jazz band where nobody is really “in charge” but the whole thing works out anyway?  
As was shown in recent years of AI research, the water level on AI rises on translation and driving, yet each of these solutions does not offer us much insight by way of making progress on all the other problems in Moravec’s landscape.

Academic Culture

AI is a human endeavour, and with that comes a slew of humanity-centric problems. 

Not inclusive, participants are shaping technology and society in ways that don’t include the perspectives of the world. As with any technological development, this poses great risk for increasing wealth inequality in the world.

Blog post on why this prof left academia is a good summary: https://reyammer.io/blog/2020/10/03/the-good-the-bad-and-the-bye-bye-why-i-left-my-tenured-academic-job/
 
Every academic will pay lip servie to “the publish/perish” culture, but then go back right to churning out  a lot of bad papers and point fingers at other academics

Academics are distracted by a lot of things that have nothing to do with research / engineering - for example, getting citations, reviewing duties, chairing conferences. The more senior someone gets, the more management bullshit they have to deal with and the lest time they have to actually build. You see a common problem where top researchers rarely have new ideas of their own - rather, they are just the mean belief over their 5 best grad students (who are the ones with actual new ideas). 

Many academic works are more art pieces than actual systems that make it into real products - often because the method does not scale to the real world, or because the effect is marginal given the addition of even more data scale, or more often than not, the researcher simply does not put in the work to make it happen.
Academic incentives do not encourage long-term investment in Manhattan-style projects. Grad students spend 1-2 years building some piece of the puzzle by themselves. Code is not being re-used.
Academic ML researchers tend to have 1 or 2 authors do the bulk of the work, with everyone else in a honorary advising role or doing a bit of paper writing (but not running actual experiments).


Generalization vs. Fabrication

Another human-centric problem that faces us today

at times even dangerously misguided.

Consider an example where we attempt to reconstruct living depictions of animals from their bones, so that we may ask what color a T-Rex may have been or whether it had feathers. 

You can build a large dataset of present-day animals (for which we know what they look like), and some birds might even have skeletons resembling prehistoric dinosaurs. You then train an ML model to predict the skin from the bones, and find that it works well. You then apply the same model to dinosaur bones, and it renders a fairly interesting interpretatiion.   

If you ask an ML system to solve a problem for which the inputs cannot possibly determine the answer (i.e. the color of a fossilized animal or the skin color from a person's statue), it will give you a highly biased answer.

How do we build a single computer that can open a soup can, drive a rental car, design a shoe, socialize with a group of strangers, and console a grieving person?

An ML system that predicts facial reconstructions of roman emperors from statues can lead to people believing these images as true, i.e. "this is what the Roman Emperors looked like". In our age of media where people are quick to re-share content without fact-checking, it can lead to computer-generated content being accepted as an authoritative interpretation, or even a historical fact. 
I fear that some laypeople think of ML as "magic software that can do anything" without considering whether a problem is non-ambiguous given its inputs (e.g. something like object classification is less ambiguous than this problem).



Note that many of these abilities are capabilities, much like we specify assumptions in the functional form of model. Some of these are internal behaviors, ones that we don’t really observe physically but believe happen in a brain. We observe the consequences of behavior like learning and memory, not the act of storage and retrieval themselves.

Note that these are behaviors we wish to see in systems, and somewhat agnostic to how we actually go about acquiring these.




<show a toy plot of two tasks>


It is clear that thinking about a “function to aprpoximate dreaming” and a “function to approximate learning” might be the wrong thing.

The database you gather of inputs and outputs rarely specifies the function that you actually want.


Imbuing functions not just to spit out predictions, but to spit out actions. This simple modification -- mapping a prediction to an external influence on the world -- is key to delineate between perceptions that predict and functions with agency

In machine learning, it is only possible to “understand” a discrete concept A from all other concepts B. This requires drawing  a “line” separating all instances of A from B, and this is exactly the process that we train neural nets to do when learning to perform classification: draw the separating boundary between two mutually exclusive concepts. 

Likewise, we must draw an imaginary boundary separating “what neural nets can do” from “what neural nets cannot do” - one cannot fully understand one without the other.


In the previous chapter we discussed how general methods can get even mroe general when they subsume the design process itself - we can go increasingly meta. While this principle singularly explains why ML works (use data, make fewer and fewer assumptions), it gets exponentially more expensive each time we attempt to go up the “meta” optimization hierarchy. 

Furthermore, even if the high level recipe is obvious, execution is everything and it may be quite challenging to formulate problems in the right way. There are countless minutiae of obstacles that appear once you get down to the implementation details. For example, multi-task systems - things that are supposed to handle multiple different types of inputs - can sometimes from catastrophic forgetting, where the acquisition of one knowledge annihilates knowledge of how to do another (as opposed to an optimistic story where they help each other):


 
Optimizing a single Decision vs. lifetime of decisions vs. entire evolutionary history of all decisions
 
Behavioral Process
Optimization Criterion
(how to make it better)
Inference Time
Training Time for improving the process
L1 Single image rollout
Differentiable loss +Feed-forward net (CNN, MLP) 
50ms
hours
L2 Single episode of experience (inference)
Sequence net (RNN, Transformer) + Differentiable loss
5s
days
L3 Single lifetime (sequence of episodes)
Very big sequence net + differentiable loss
500s
weeks
L4 Generations
(Sequence of lifetimes) 
Evolutionary Strategies / Meta-Optimization
(desirable evolutionary trajectory)
50000s
months/years

Research Loop 



Human-in-the-loop, think about experiment result and try again (is a multiplier on any of the past)


Length of experiment
centuries

 
I'm already making a fairly unrealistic assumption that "episodes" are only 100 frames, and that while each inference time is 100x the inference time of the previous step, the training time only scales 10x of the prior training time (maybe thanks to better meta-learning algorithms or dynamic programming ideas)
 
The only respite would be if the first 4 phases were done on small surrogate problems and the final thing was done once. Or if there was a way to not have to re-start life several times over in the process of attempting to optimize it. 


what would the operating system look like? 



I am concerned with creating 2.0 systems, because I am not particularly interested in AI that understands itself so well it can design itself. I




You can build a model that exhibits knowledge of the world - by asking it to fill in the blanks to written text, by asking it to fill in pixels, by asking it to predict what a human would do. But in service of what? There is nothing that forces the model to be internally consistent 


Better Methods for Policy Search/Optimization: There is a distinction between the behavior (we wish to see memory, learning, inference, etc) and the means by which we optimize for it (policy search). Self-play, imitation learning, RL are all methods to achieve it. Self-play games will be a key component of this, because we have guaranteed improvement (as opposed to degenerate equilibria), but we can also use as many tricks as we have our disposal to accelerate learning. 

This encompasses imitation learning, reinforcement learning.

Real safety issues:
https://twitter.com/dogryan100/status/1322001195532128256?s=20

Incident where a NaN value was propagated to the steering lock, setting its steering value to the rightmost value, causing it to hard swerve into a wall. 

This is not a failure of AI (it’s not even clear whether deep learning was even used here) - but rather the fact that programming is hard, building complex systems is hard, and often there are unforeseen consequences when building robots that interact with the real world.

